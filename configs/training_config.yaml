# Model settings
base_model: "mlx-community/Llama-3.2-1B-Instruct-4bit"
output_dir: "models/fine_tuned/my_assistant"

# Data
train_data: "data/processed/training_data_train.jsonl"
val_data: "data/processed/training_data_val.jsonl"

# LoRA settings
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
lora_layers: 16  # Number of layers to apply LoRA to

# Training hyperparameters
learning_rate: 0.0001
batch_size: 2
num_epochs: 3
warmup_steps: 100
max_seq_length: 512

# Optimization
grad_accumulation_steps: 4  # Effective batch size = batch_size * grad_accumulation_steps
weight_decay: 0.01

# Logging
save_every: 100
eval_every: 50
log_every: 10